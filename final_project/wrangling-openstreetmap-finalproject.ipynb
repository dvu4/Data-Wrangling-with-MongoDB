{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling  OpenStreetMap Data  with MongoDB\n",
    "###  by Duc Vu in fulfillment of Udacityâ€™s Data Analyst Nanodegree, Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap is an open project that lets eveyone use and create a free editable map of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chosen Map Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I choose to analyze data from Boston, Massachusetts want to show you to fix one type of error, that is the address of the street. And not only that, I also will show you how to put the data that has been audited into MongoDB instance. We also use MongoDB's Agregation Framework to get overview and analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" src=\"http://www.openstreetmap.org/export/embed.html?bbox=-71.442,42.1858,-70.6984,42.4918&amp;layer=mapnik\"></iframe><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"425\" height=\"350\" frameborder=\"0\" scrolling=\"no\" marginheight=\"0\" marginwidth=\"0\" \\\n",
    "src=\"http://www.openstreetmap.org/export/embed.html?bbox=-71.442,42.1858,-70.6984,42.4918&amp;layer=mapnik\"></iframe><br/>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The dataset is here https://s3.amazonaws.com/metro-extracts.mapzen.com/boston_massachusetts.osm.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'boston_massachusetts.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the Overpass API to download the OpenStreetMap XML for the corresponding bounding box:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Auditing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this project, I will parse through the downloaded OSM XML file with ElementTree and find the number of each type of element since the XML file are too large to work with in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 9586,\n",
      " 'nd': 2242045,\n",
      " 'node': 1886391,\n",
      " 'osm': 1,\n",
      " 'relation': 1186,\n",
      " 'tag': 846441,\n",
      " 'way': 294246}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    '''\n",
    "    this function will return a dictionary with the tag name as the key \n",
    "    and number of times this tag can be encountered in the map as value.\n",
    "    '''\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag] +=1\n",
    "        else:\n",
    "            tags[elem.tag]= 1\n",
    "                \n",
    "    return tags\n",
    "tags = count_tags(filename)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing the data and add it into MongoDB, I should check the \"k\" value for each 'tag' and see if they can be valid keys in MongoDB, as well as see if there are any other potential problems.\n",
    "\n",
    "I have built 3 regular expressions to check for certain patterns in the tags to change the data model\n",
    "and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "\n",
    "\n",
    "Here are three regular expressions: lower, lower_colon, and problemchars.\n",
    "\n",
    "- lower: matches strings containing lower case characters\n",
    "- lower_colon: matches strings containing lower case characters and a single colon within the string\n",
    "- problemchars: matches characters that cannot be used within keys in MongoDB\n",
    " \n",
    " \n",
    "- example: {\"address\": {\"street\": \"Some value\"}}\n",
    "\n",
    "So, we have to see if we have such tags, and if we have any tags with problematic characters.\n",
    "Please complete the function 'key_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 749756, 'lower_colon': 56532, 'other': 40149, 'problemchars': 4}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    '''\n",
    "     this function counts number of times the unusual tag element can be encountered in the map.\n",
    "    Args:\n",
    "        element(string): tag element in the map.\n",
    "        keys(int): number of that encountered tag in the map\n",
    "    '''\n",
    "    if element.tag == \"tag\":\n",
    "\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys[\"problemchars\"] +=1\n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "        \n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    '''\n",
    "     this function will return a dictionary with the unexpexted tag element as the key \n",
    "     and number of times this string can be encountered in the map as value.\n",
    "    Args:\n",
    "        filename(osm): openstreetmap file.\n",
    "    '''\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "'''\n",
    "#Below unit testing runs process_map with file example.osm\n",
    "def test():\n",
    "    keys = process_map('example.osm')\n",
    "    pprint.pprint(keys)\n",
    "    assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "\n",
    "'''\n",
    "\n",
    "keys = process_map(filename)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will redefine process_map to build a set of unique userid's found within the XML. I will then output the length of this set, representing the number of unique users making edits in the chosen map area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    }
   ],
   "source": [
    "def process_map(filename):\n",
    "    '''\n",
    "    This function will return a set of unique user IDs (\"uid\") \n",
    "    making edits in the chosen map area (i.e Boston area).\n",
    "    Args:\n",
    "        filename(osm): openstreetmap file.\n",
    "    '''\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        #print element.attrib\n",
    "        \n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        '''\n",
    "        if \"uid\" in element.attrib:\n",
    "            users.add(element.attrib['uid'])\n",
    "        '''\n",
    "    return users\n",
    "\n",
    "'''\n",
    "#Below unit testing runs process_map with file example.osm\n",
    "def test():\n",
    "\n",
    "    users = process_map('example.osm')\n",
    "    pprint.pprint(users)\n",
    "    assert len(users) == 6\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "'''\n",
    "\n",
    "    \n",
    "    \n",
    "users = process_map(filename)\n",
    "#pprint.pprint(users)\n",
    "print len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Street name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of this project will be devoted to auditing and cleaning street names in the OSM XML file by changing the variable 'mapping' to reflect the changes needed to fix the unexpected or abbreviated street types to the appropriate ones in the expected list. I will find these abbreviations and replace them with their full text form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name, rex):\n",
    "    '''\n",
    "     This function will take in the dictionary of street types, a string of street name to audit, \n",
    "     a regex to match against that string, and the list of expected street types.\n",
    "     Args:\n",
    "        street_types(dictionary): dictionary of street types.\n",
    "        street_name(string):  a string of street name to audit.\n",
    "        rex(regex): a compiled regular expression to match against the street_name.\n",
    "    '''\n",
    "    #m = street_type_re.search(street_name)\n",
    "    m = rex.search(street_name)\n",
    "    #print m\n",
    "    #print m.group()\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that not only audits tag elements where k=\"addr:street\", but whichever tag elements match the is_street_name function. The audit function also takes in a regex and the list of expected matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile,rex):\n",
    "    '''\n",
    "     This function changes the variable 'mapping' to reflect the changes needed to fix\n",
    "    the unexpected street types to the appropriate ones in the expected list.\n",
    "     Args:\n",
    "        filename(osm): openstreetmap file.\n",
    "        rex(regex): a compiled regular expression to match against the street_name.\n",
    "    '''\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'],rex)\n",
    "\n",
    "    return street_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is_street_name determines if an element contains an attribute k=\"addr:street\". I will use is_street_name as the tag filter when I call the audit function to audit street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the output of audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1100': set(['First Street, Suite 1100']),\n",
      " '1702': set(['Franklin Street, Suite 1702']),\n",
      " '303': set(['First Street, Suite 303']),\n",
      " '6': set(['South Station, near Track 6']),\n",
      " '846028': set(['PO Box 846028']),\n",
      " 'Artery': set(['Southern Artery']),\n",
      " 'Ave': set(['360 Huntington Ave',\n",
      "             '738 Commonwealth Ave',\n",
      "             'Blue Hill Ave',\n",
      "             'Boston Ave',\n",
      "             'College Ave',\n",
      "             'Commonwealth Ave',\n",
      "             'Concord Ave',\n",
      "             'Francesca Ave',\n",
      "             'Harrison Ave',\n",
      "             'Highland Ave',\n",
      "             'Huntington Ave',\n",
      "             'Josephine Ave',\n",
      "             'Lexington Ave',\n",
      "             'Massachusetts Ave',\n",
      "             'Morrison Ave',\n",
      "             'Mystic Ave',\n",
      "             'Sagamore Ave',\n",
      "             'Somerville Ave',\n",
      "             \"St. Paul's Ave\",\n",
      "             'Washington Ave',\n",
      "             'Western Ave',\n",
      "             'Willow Ave']),\n",
      " 'Ave.': set(['Brighton Ave.',\n",
      "              'Huntington Ave.',\n",
      "              'Massachusetts Ave.',\n",
      "              'Somerville Ave.',\n",
      "              'Spaulding Ave.']),\n",
      " 'Boylston': set(['Boylston']),\n",
      " 'Broadway': set(['Broadway', 'West Broadway']),\n",
      " 'Brook': set(['Furnace Brook']),\n",
      " 'Cambrdige': set(['Cambrdige']),\n",
      " 'Center': set(['Cambridge Center']),\n",
      " 'Circle': set(['Edgewood Circle']),\n",
      " 'Corner': set(['Webster Street, Coolidge Corner']),\n",
      " 'Ct': set(['Kelley Ct']),\n",
      " 'Elm': set(['Elm']),\n",
      " 'Federal': set(['Federal']),\n",
      " 'Fellsway': set(['Fellsway']),\n",
      " 'Fenway': set(['Fenway']),\n",
      " 'Floor': set(['Boylston Street, 5th Floor']),\n",
      " 'HIghway': set(['American Legion HIghway']),\n",
      " 'Hall': set(['Faneuil Hall']),\n",
      " 'Hampshire': set(['Hampshire']),\n",
      " 'Highway': set(['American Legion Highway',\n",
      "                 'Cummins Highway',\n",
      "                 \"Monsignor O'Brien Highway\",\n",
      "                 'Providence Highway',\n",
      "                 'Santilli Highway']),\n",
      " 'Holland': set(['Holland']),\n",
      " 'Hwy': set([\"Monsignor O'Brien Hwy\"]),\n",
      " 'LEVEL': set(['LOMASNEY WAY, ROOF LEVEL']),\n",
      " 'Lafayette': set(['Avenue De Lafayette']),\n",
      " 'Mall': set(['Cummington Mall']),\n",
      " 'Newbury': set(['Newbury']),\n",
      " 'Park': set(['Austin Park',\n",
      "              'Batterymarch Park',\n",
      "              'Canal Park',\n",
      "              'Exeter Park',\n",
      "              'Giles Park',\n",
      "              'Malden Street Park',\n",
      "              'Monument Park']),\n",
      " 'Pkwy': set(['Birmingham Pkwy']),\n",
      " 'Pl': set(['Longfellow Pl']),\n",
      " 'Rd': set(['Abby Rd',\n",
      "            'Aberdeen Rd',\n",
      "            'Bristol Rd',\n",
      "            'Goodnough Rd',\n",
      "            'Oakland Rd',\n",
      "            'Soldiers Field Rd',\n",
      "            'Squanto Rd']),\n",
      " 'Row': set(['Assembly Row', 'East India Row', 'Professors Row']),\n",
      " 'ST': set(['Newton ST']),\n",
      " 'South': set(['Charles Street South']),\n",
      " 'Sq.': set(['1 Kendall Sq.']),\n",
      " 'St': set(['1629 Cambridge St',\n",
      "            '644 Beacon St',\n",
      "            'Adams St',\n",
      "            'Antwerp St',\n",
      "            'Arsenal St',\n",
      "            'Athol St',\n",
      "            'Bagnal St',\n",
      "            'Beacon St',\n",
      "            'Brentwood St',\n",
      "            'Broad St',\n",
      "            'Cambridge St',\n",
      "            'Congress St',\n",
      "            'Court St',\n",
      "            'Cummington St',\n",
      "            'Dane St',\n",
      "            'Duval St',\n",
      "            'E 4th St',\n",
      "            'Elm St',\n",
      "            'Everett St',\n",
      "            'George St',\n",
      "            'Grove St',\n",
      "            'Hampshire St',\n",
      "            'Holton St',\n",
      "            'Kirkland St',\n",
      "            'Leighton St',\n",
      "            'Litchfield St',\n",
      "            'Lothrop St',\n",
      "            'Mackin St',\n",
      "            'Maverick St',\n",
      "            'Medford St',\n",
      "            'Merrill St',\n",
      "            'N Beacon St',\n",
      "            'Newbury St',\n",
      "            'Norfolk St',\n",
      "            'Portsmouth St',\n",
      "            'Richardson St',\n",
      "            'Salem St',\n",
      "            'Sea St',\n",
      "            'South Waverly St',\n",
      "            'Stewart St',\n",
      "            'Summer St',\n",
      "            'Ware St',\n",
      "            'Waverly St',\n",
      "            'Winter St']),\n",
      " 'St,': set(['Walnut St,']),\n",
      " 'St.': set(['Albion St.',\n",
      "             'Banks St.',\n",
      "             'Boylston St.',\n",
      "             'Brookline St.',\n",
      "             'Centre St.',\n",
      "             'Elm St.',\n",
      "             'Main St.',\n",
      "             'Marshall St.',\n",
      "             'Maverick St.',\n",
      "             'Pearl St.',\n",
      "             'Prospect St.',\n",
      "             \"Saint Mary's St.\",\n",
      "             'Stuart St.',\n",
      "             'Tremont St.']),\n",
      " 'Street.': set(['Hancock Street.']),\n",
      " 'Terrace': set(['Alberta Terrace', 'Norfolk Terrace', 'Westbourne Terrace']),\n",
      " 'Way': set(['Artisan Way',\n",
      "             'Courthouse Way',\n",
      "             'David G Mugar Way',\n",
      "             'Davidson Way',\n",
      "             'Evans Way',\n",
      "             'Harry Agganis Way',\n",
      "             'Ross Way',\n",
      "             'Yawkey Way']),\n",
      " 'Wharf': set(['Long Wharf', 'Rowes Wharf']),\n",
      " 'Windsor': set(['Windsor']),\n",
      " 'Winsor': set(['Winsor']),\n",
      " 'ave': set(['Massachusetts ave']),\n",
      " 'floor': set(['First Street, 18th floor', 'Sidney Street, 2nd floor']),\n",
      " 'place': set(['argus place']),\n",
      " 'rd.': set(['Corey rd.']),\n",
      " 'st': set(['Church st']),\n",
      " 'street': set(['Boston street'])}\n"
     ]
    }
   ],
   "source": [
    "st_types = audit(filename, rex = street_type_re)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the audit, I will create a dictionary to map abbreviations to their full, clean representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"ave\" : \"Avenue\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Ct\" : \"Court\",\n",
    "            \"HIghway\": \"Highway\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"LEVEL\": \"Level\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"rd.\" : \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Sq.\" : \"Square\", \n",
    "            \"st\": \"Street\",\n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"St,\": \"Street\", \n",
    "            \"ST\": \"Street\",\n",
    "            \"Street.\" : \"Street\",               \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first result of audit gives me a list of some abbreviated street types (as well as unexpected clean street types, cardinal directions, and highway numbers). So I need to build an update_name function to replace these abbreviated street types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping,rex):\n",
    "    '''\n",
    "     This function takes a string with street name as an argument \n",
    "     and replace these abbreviated street types with the fixed name.\n",
    "     Args:\n",
    "        name(string): street name to update.\n",
    "        mapping(dictionary): a mapping dictionary.\n",
    "        rex(regex): a compiled regular expression to match against the street_name.\n",
    "    '''\n",
    "\n",
    "    #m = street_type_re.search(name)\n",
    "    m = rex.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        new_street_type = mapping[street_type]\n",
    "        name = re.sub(rex, new_street_type, name) # re.sub(old_pattern, new_pattern, file)\n",
    "        #name = street_type_re.sub(new_street_type, name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this update_name works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walnut St, => Walnut Street\n",
      "Maverick St. => Maverick Street\n",
      "Pearl St. => Pearl Street\n",
      "Banks St. => Banks Street\n",
      "Tremont St. => Tremont Street\n",
      "Centre St. => Centre Street\n",
      "Marshall St. => Marshall Street\n",
      "Prospect St. => Prospect Street\n",
      "Main St. => Main Street\n",
      "Albion St. => Albion Street\n",
      "Saint Mary's St. => Saint Mary's Street\n",
      "Boylston St. => Boylston Street\n",
      "Stuart St. => Stuart Street\n",
      "Elm St. => Elm Street\n",
      "Brookline St. => Brookline Street\n",
      "Oakland Rd => Oakland Road\n",
      "Abby Rd => Abby Road\n",
      "Bristol Rd => Bristol Road\n",
      "Squanto Rd => Squanto Road\n",
      "Goodnough Rd => Goodnough Road\n",
      "Soldiers Field Rd => Soldiers Field Road\n",
      "Aberdeen Rd => Aberdeen Road\n",
      "Massachusetts ave => Massachusetts Avenue\n",
      "Newton ST => Newton Street\n",
      "Longfellow Pl => Longfellow Place\n",
      "Hancock Street. => Hancock Street\n",
      "Monsignor O'Brien Hwy => Monsignor O'Brien Highway\n",
      "Corey rd. => Corey Road\n",
      "Brighton Ave. => Brighton Avenue\n",
      "Spaulding Ave. => Spaulding Avenue\n",
      "Massachusetts Ave. => Massachusetts Avenue\n",
      "Somerville Ave. => Somerville Avenue\n",
      "Huntington Ave. => Huntington Avenue\n",
      "LOMASNEY WAY, ROOF LEVEL => LOMASNEY WAY, ROOF Level\n",
      "Salem St => Salem Street\n",
      "Brentwood St => Brentwood Street\n",
      "Medford St => Medford Street\n",
      "Athol St => Athol Street\n",
      "Everett St => Everett Street\n",
      "South Waverly St => South Waverly Street\n",
      "Litchfield St => Litchfield Street\n",
      "Hampshire St => Hampshire Street\n",
      "George St => George Street\n",
      "Winter St => Winter Street\n",
      "Broad St => Broad Street\n",
      "Cambridge St => Cambridge Street\n",
      "Arsenal St => Arsenal Street\n",
      "Merrill St => Merrill Street\n",
      "Maverick St => Maverick Street\n",
      "Antwerp St => Antwerp Street\n",
      "Beacon St => Beacon Street\n",
      "1629 Cambridge St => 1629 Cambridge Street\n",
      "E 4th St => E 4th Street\n",
      "Elm St => Elm Street\n",
      "Congress St => Congress Street\n",
      "Lothrop St => Lothrop Street\n",
      "Stewart St => Stewart Street\n",
      "Dane St => Dane Street\n",
      "Norfolk St => Norfolk Street\n",
      "Bagnal St => Bagnal Street\n",
      "Cummington St => Cummington Street\n",
      "Holton St => Holton Street\n",
      "Mackin St => Mackin Street\n",
      "Waverly St => Waverly Street\n",
      "Court St => Court Street\n",
      "Summer St => Summer Street\n",
      "Duval St => Duval Street\n",
      "Kirkland St => Kirkland Street\n",
      "Adams St => Adams Street\n",
      "644 Beacon St => 644 Beacon Street\n",
      "N Beacon St => N Beacon Street\n",
      "Grove St => Grove Street\n",
      "Leighton St => Leighton Street\n",
      "Richardson St => Richardson Street\n",
      "Newbury St => Newbury Street\n",
      "Sea St => Sea Street\n",
      "Ware St => Ware Street\n",
      "Portsmouth St => Portsmouth Street\n",
      "Massachusetts Ave => Massachusetts Avenue\n",
      "Highland Ave => Highland Avenue\n",
      "Lexington Ave => Lexington Avenue\n",
      "Huntington Ave => Huntington Avenue\n",
      "Francesca Ave => Francesca Avenue\n",
      "Willow Ave => Willow Avenue\n",
      "360 Huntington Ave => 360 Huntington Avenue\n",
      "Harrison Ave => Harrison Avenue\n",
      "Somerville Ave => Somerville Avenue\n",
      "Mystic Ave => Mystic Avenue\n",
      "Blue Hill Ave => Blue Hill Avenue\n",
      "Washington Ave => Washington Avenue\n",
      "Morrison Ave => Morrison Avenue\n",
      "Boston Ave => Boston Avenue\n",
      "738 Commonwealth Ave => 738 Commonwealth Avenue\n",
      "Josephine Ave => Josephine Avenue\n",
      "Sagamore Ave => Sagamore Avenue\n",
      "Commonwealth Ave => Commonwealth Avenue\n",
      "St. Paul's Ave => St. Paul's Avenue\n",
      "Concord Ave => Concord Avenue\n",
      "Western Ave => Western Avenue\n",
      "College Ave => College Avenue\n",
      "Birmingham Pkwy => Birmingham Parkway\n",
      "Church st => Church Street\n",
      "1 Kendall Sq. => 1 Kendall Square\n",
      "American Legion HIghway => American Legion Highway\n",
      "Kelley Ct => Kelley Court\n"
     ]
    }
   ],
   "source": [
    "for st_type, ways in st_types.iteritems():\n",
    "    if st_type in mapping:\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping, rex = street_type_re)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all the abbreviated street types updated as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cardinal direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I can see there is still an issue: cardinal directions (North, South, East, and West) appear to be universally abbreviated.  Therefore , I will traverse the cardinal_directions dictionary and apply the updates for both street type and cardinal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cardinal_dir_re = re.compile(r'^[NSEW]\\b\\.?', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result of audit the cardinal directions with this new regex 'cardinal_dir_re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': set(['E 4th St', 'E Elm Avenue']), 'N': set(['N Beacon St'])}\n"
     ]
    }
   ],
   "source": [
    "dir_st_types = audit(filename, rex = cardinal_dir_re)\n",
    "pprint.pprint(dict(dir_st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a dictionary to map abbreviations (N, S, E and W) to their full representations of cardinal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinal_directions_mapping = \\\n",
    "    {\n",
    "        \"E\" : \"East\",\n",
    "        \"N\" : \"North\",\n",
    "        \"S\" : \"South\",\n",
    "        \"W\" : \"West\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look like all expected cardinal directions have been replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Elm Avenue => East Elm Avenue\n",
      "E 4th St => East 4th St\n",
      "N Beacon St => North Beacon St\n"
     ]
    }
   ],
   "source": [
    "for st_type, ways in dir_st_types.iteritems():\n",
    "    if st_type in cardinal_directions_mapping:\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, cardinal_directions_mapping, rex = cardinal_dir_re)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 Postal codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exam the postal codes, we can see that there are still some invalid postal codes, so we also need to clean postal codes. I will use regular expressions to identify invalid postal codes and return standardized results. For example, if postal codes like 'MA 02131-4931' and '02131-2460' should be mapped to '02131'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "badZipCode = [\"MA\", \"Mass Ave\"]\n",
    "\n",
    "\n",
    "zip_code_re = re.compile(r\"(\\d{5})(-\\d{4})?$\") #5 digits in a row @ end of string\n",
    "                                           #and optionally dash plus 4 digits\n",
    "                                           #return different parts of the match and an optional clause (?) \n",
    "                                           #for the dash and four digits at the end of the string ($)\n",
    "\n",
    "# find the zipcodes\n",
    "def get_postcode(element):\n",
    "    if (element.attrib['k'] == \"addr:postcode\"):\n",
    "        postcode = element.attrib['v']\n",
    "        return postcode\n",
    "\n",
    "    \n",
    "# update zipcodes\n",
    "def update_postal(postcode, rex):\n",
    "     '''\n",
    "     This function takes a string with zip code as an argument \n",
    "     and replace these wrong zip code with the fixed zip code.\n",
    "     Args:\n",
    "        postcode(string): zip code to update.\n",
    "        rex(regex): a compiled regular expression to match against the zip code.\n",
    "    '''\n",
    "    if postcode is not None:\n",
    "        zip_code = re.search(rex,postcode)\n",
    "        if zip_code:\n",
    "            postcode = zip_code.group(1)\n",
    "                   \n",
    "    return postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    '''\n",
    "     This function return a dictionary with the key is the zip code \n",
    "     and the value is the number of that zip code in osm file.\n",
    "     Args:\n",
    "        filename(osm): openstreetmap file.\n",
    "    '''\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    data_dict = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        \n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if get_postcode(tag):\n",
    "                    postcode = get_postcode(tag)\n",
    "                    data_dict[postcode] += 1 \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' 02472': 1,\n",
      " '01125': 1,\n",
      " '01238': 1,\n",
      " '01240': 1,\n",
      " '01250': 1,\n",
      " '01821': 1,\n",
      " '01944': 1,\n",
      " '02026': 1,\n",
      " '02026-5036': 1,\n",
      " '02043': 3,\n",
      " '02108': 11,\n",
      " '02109': 8,\n",
      " '02110': 7,\n",
      " '02110-1301': 1,\n",
      " '02111': 16,\n",
      " '02113': 1,\n",
      " '02114': 58,\n",
      " '02114-3203': 1,\n",
      " '02115': 9,\n",
      " '02116': 41,\n",
      " '02118': 5,\n",
      " '02119': 1,\n",
      " '02120': 3,\n",
      " '02121': 2,\n",
      " '02122': 6,\n",
      " '02124': 8,\n",
      " '02125': 5,\n",
      " '02126': 7,\n",
      " '02127': 11,\n",
      " '02128': 22,\n",
      " '02129': 2,\n",
      " '02130': 43,\n",
      " '02130-4803': 1,\n",
      " '02131': 8,\n",
      " '02131-3025': 2,\n",
      " '02131-4931': 1,\n",
      " '02132': 17,\n",
      " '02132-1239': 1,\n",
      " '02132-3226': 1,\n",
      " '02134': 48,\n",
      " '02134-1305': 9,\n",
      " '02134-1306': 2,\n",
      " '02134-1307': 29,\n",
      " '02134-1311': 4,\n",
      " '02134-1312': 2,\n",
      " '02134-1313': 4,\n",
      " '02134-1316': 3,\n",
      " '02134-1317': 4,\n",
      " '02134-1318': 2,\n",
      " '02134-1319': 5,\n",
      " '02134-1321': 4,\n",
      " '02134-1322': 4,\n",
      " '02134-1327': 1,\n",
      " '02134-1409': 4,\n",
      " '02134-1420': 9,\n",
      " '02134-1433': 11,\n",
      " '02134-1442': 5,\n",
      " '02135': 249,\n",
      " '02136': 6,\n",
      " '02136-2460': 1,\n",
      " '02138': 49,\n",
      " '02138-1901': 1,\n",
      " '02138-2701': 8,\n",
      " '02138-2706': 3,\n",
      " '02138-2724': 1,\n",
      " '02138-2735': 1,\n",
      " '02138-2736': 2,\n",
      " '02138-2742': 1,\n",
      " '02138-2762': 1,\n",
      " '02138-2763': 1,\n",
      " '02138-2801': 4,\n",
      " '02138-2901': 4,\n",
      " '02138-2903': 8,\n",
      " '02138-2933': 3,\n",
      " '02138-3003': 1,\n",
      " '02138-3824': 1,\n",
      " '02139': 227,\n",
      " '02140': 14,\n",
      " '02140-1340': 1,\n",
      " '02140-2215': 1,\n",
      " '02141': 16,\n",
      " '02142': 31,\n",
      " '02143': 50,\n",
      " '02144': 89,\n",
      " '02145': 18,\n",
      " '02148': 1,\n",
      " '02149': 15,\n",
      " '02150': 8,\n",
      " '02151': 4,\n",
      " '02152': 2,\n",
      " '02155': 22,\n",
      " '02159': 1,\n",
      " '02169': 52,\n",
      " '02170': 3,\n",
      " '02171': 5,\n",
      " '02174': 1,\n",
      " '02184': 3,\n",
      " '02186': 8,\n",
      " '02205': 1,\n",
      " '02210': 26,\n",
      " '02215': 59,\n",
      " '02228': 1,\n",
      " '02284-6028': 1,\n",
      " '02445': 11,\n",
      " '02445-5841': 1,\n",
      " '02446': 36,\n",
      " '02458': 4,\n",
      " '02459': 5,\n",
      " '02467': 23,\n",
      " '02472': 29,\n",
      " '02474': 21,\n",
      " '02474-8735': 1,\n",
      " '02476': 8,\n",
      " '02478': 9,\n",
      " 'MA': 6,\n",
      " 'MA 02116': 4,\n",
      " 'MA 02186': 1,\n",
      " 'Mass Ave': 1}\n"
     ]
    }
   ],
   "source": [
    "zip_code_types = audit(filename)\n",
    "pprint.pprint(dict(zip_code_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02186 => 02186\n",
      "02184 => 02184\n",
      "02134-1327 => 02134\n",
      "02130 => 02130\n",
      "02134-1322 => 02134\n",
      "02134-1321 => 02134\n",
      "02138-1901 => 02138\n",
      "02132-3226 => 02132\n",
      "01821 => 01821\n",
      "02134-1433 => 02134\n",
      "02108 => 02108\n",
      "02026 => 02026\n",
      "02476 => 02476\n",
      "02474 => 02474\n",
      "02472 => 02472\n",
      "02139 => 02139\n",
      "02134-1319 => 02134\n",
      "02478 => 02478\n",
      "02136-2460 => 02136\n",
      "02131-3025 => 02131\n",
      "02136 => 02136\n",
      "02140-1340 => 02140\n",
      "02134 => 02134\n",
      "02205 => 02205\n",
      "02132 => 02132\n",
      "02131 => 02131\n",
      " 02472 => 02472\n",
      "02110-1301 => 02110\n",
      "02138 => 02138\n",
      "02138-2903 => 02138\n",
      "02138-2901 => 02138\n",
      "02134-1442 => 02134\n",
      "01250 => 01250\n",
      "02132-1239 => 02132\n",
      "02446 => 02446\n",
      "02445 => 02445\n",
      "02138-2742 => 02138\n",
      "02120 => 02120\n",
      "02121 => 02121\n",
      "02210 => 02210\n",
      "02124 => 02124\n",
      "02125 => 02125\n",
      "02126 => 02126\n",
      "02215 => 02215\n",
      "02128 => 02128\n",
      "02129 => 02129\n",
      "02474-8735 => 02474\n",
      "01240 => 01240\n",
      "02114-3203 => 02114\n",
      "02458 => 02458\n",
      "02459 => 02459\n",
      "MA 02116 => 02116\n",
      "01125 => 01125\n",
      "02109 => 02109\n",
      "02228 => 02228\n",
      "MA 02186 => 02186\n",
      "02155 => 02155\n",
      "02151 => 02151\n",
      "02150 => 02150\n",
      "02152 => 02152\n",
      "02159 => 02159\n",
      "02026-5036 => 02026\n",
      "01238 => 01238\n",
      "02138-2724 => 02138\n",
      "02445-5841 => 02445\n",
      "02138-2933 => 02138\n",
      "02144 => 02144\n",
      "02145 => 02145\n",
      "02142 => 02142\n",
      "02143 => 02143\n",
      "02140 => 02140\n",
      "02141 => 02141\n",
      "02148 => 02148\n",
      "02149 => 02149\n",
      "02138-2735 => 02138\n",
      "02138-2736 => 02138\n",
      "02131-4931 => 02131\n",
      "02138-3824 => 02138\n",
      "02134-1316 => 02134\n",
      "02134-1317 => 02134\n",
      "02134-1312 => 02134\n",
      "02134-1313 => 02134\n",
      "02134-1311 => 02134\n",
      "02134-1318 => 02134\n",
      "01944 => 01944\n",
      "02171 => 02171\n",
      "02134-1409 => 02134\n",
      "02174 => 02174\n",
      "02170 => 02170\n",
      "02122 => 02122\n",
      "02138-2706 => 02138\n",
      "02138-2701 => 02138\n",
      "02140-2215 => 02140\n",
      "02134-1420 => 02134\n",
      "02127 => 02127\n",
      "02114 => 02114\n",
      "02134-1305 => 02134\n",
      "02134-1307 => 02134\n",
      "02134-1306 => 02134\n",
      "02043 => 02043\n",
      "02169 => 02169\n",
      "02130-4803 => 02130\n",
      "02138-2801 => 02138\n",
      "02284-6028 => 02284\n",
      "02119 => 02119\n",
      "02118 => 02118\n",
      "02138-3003 => 02138\n",
      "02111 => 02111\n",
      "02110 => 02110\n",
      "02113 => 02113\n",
      "02115 => 02115\n",
      "02135 => 02135\n",
      "02116 => 02116\n",
      "02467 => 02467\n",
      "02138-2762 => 02138\n",
      "02138-2763 => 02138\n"
     ]
    }
   ],
   "source": [
    "for raw_zip_code in zip_code_types:\n",
    "    if raw_zip_code not in badZipCode:\n",
    "        better_zip_code = update_postal(raw_zip_code, rex = zip_code_re)\n",
    "        print raw_zip_code, \"=>\", better_zip_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 The total number of nodes and ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will count the total number of nodes and ways that contain a tag child with k=\"addr:street\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2976"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_file = open(filename, \"r\")\n",
    "address_count = 0\n",
    "\n",
    "for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"): \n",
    "            if is_street_name(tag):\n",
    "                address_count += 1\n",
    "\n",
    "address_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing for MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing the XML data into MongoDB, I have to transform the shape of data into json documents structured (a list of dictionaries) like this"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {         \n",
    "          \"version\":\"2\",          \n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",          \n",
    "          \"user\":\"linuxUser16\",          \n",
    "          \"uid\":\"1219059\"        \n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"        \n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the rules:\n",
    "- process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings.\n",
    "- if second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it\n",
    "  same as any other tag.\n",
    "- if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "  should be turned into:\n",
    "{...\n",
    "\"address\": {    \n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "- for \"way\" specifically:\n",
    "  <nd ref=\"305896090\"/>  \n",
    "  <nd ref=\"1719825889\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "should be turned into"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# \"node_refs\": [\"305896090\", \"1719825889\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    '''\n",
    "     This function will parse the map file and return a dictionary, \n",
    "     containing the shaped data for that element.\n",
    "     Args:\n",
    "        element(string): element in the map.\n",
    "    '''\n",
    "    node = {}\n",
    "    # create an address dictionary\n",
    "    address = {}\n",
    "    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # YOUR CODE HERE\n",
    "        node[\"type\"] = element.tag\n",
    "\n",
    "        #for key in element.attrib.keys()\n",
    "        for key in element.attrib:\n",
    "            #print key\n",
    "\n",
    "            if key in CREATED:\n",
    "                if \"created\" not in node:\n",
    "                    node[\"created\"] = {}\n",
    "                node[\"created\"][key] = element.attrib[key]\n",
    "\n",
    "            elif key in [\"lat\",\"lon\"]:\n",
    "                if \"pos\" not in node:\n",
    "                    node[\"pos\"] = [None, None]\n",
    "                if key == \"lat\":\n",
    "                    node[\"pos\"][0] = float(element.attrib[key])\n",
    "                elif key == \"lon\":\n",
    "                    node[\"pos\"][1] = float(element.attrib[key])\n",
    "            else:\n",
    "                node[key] = element.attrib[key]\n",
    "\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                tag_key = tag.attrib[\"k\"]   # key\n",
    "                tag_value = tag.attrib[\"v\"] # value\n",
    "                if not problemchars.match(tag_key):\n",
    "                    if tag_key.startswith(\"addr:\"):# Single colon beginning with addr\n",
    "                        if \"address\" not in node:\n",
    "                            node[\"address\"] = {}\n",
    "                        sub_addr = tag_key[len(\"addr:\"):]\n",
    "                        if not lower_colon.match(sub_addr): # Tags with no colon\n",
    "                            address[sub_addr] = tag_value \n",
    "                            node[\"address\"] = address\n",
    "                            #node[\"address\"][sub_addr] = tag_value\n",
    "                    elif lower_colon.match(tag_key): # Single colon not beginnning with \"addr:\"\n",
    "                        node[tag_key] = tag_value\n",
    "                    else:\n",
    "                        node[tag_key] = tag_value # Tags with no colon, not beginnning with \"addr:\"\n",
    "\n",
    "        for nd in element.iter(\"nd\"):\n",
    "            if \"node_refs\" not in node:\n",
    "                node[\"node_refs\"] = []\n",
    "            node[\"node_refs\"].append(nd.attrib[\"ref\"])\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to parse the XML, shape the elements, and write to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "process_map(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of XML and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The downloaded XML file is 421.230253 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print \"The downloaded XML file is {} MB\".format(os.path.getsize(filename)/1.0e6) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The json file is 484.237902 MB\n"
     ]
    }
   ],
   "source": [
    "print \"The json file is {} MB\".format(os.path.getsize(filename + \".json\")/1.0e6) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute mongod to run MongoDB\n",
    "Use the subprocess module to run shell commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "import subprocess\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen(\"mongod\", preexec_fn = os.setsid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://sharats.me/the-ever-useful-and-neat-subprocess-module.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect database with pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = \"osm\"\n",
    "\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to import a large amounts of data, mongoimport is recommended.\n",
    "\n",
    "First build a mongoimport command, then use subprocess.call to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping collection\n",
      "Executing: mongoimport --db osm --collection boston_massachusetts --file /Users/ducvu/Documents/ud032-master/final_project/boston_massachusetts.osm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build mongoimport command\n",
    "collection = filename[:filename.find(\".\")]\n",
    "#print collection\n",
    "working_directory = \"/Users/ducvu/Documents/ud032-master/final_project/\"\n",
    "\n",
    "json_file = filename + \".json\"\n",
    "#print json_file\n",
    "mongoimport_cmd = \"mongoimport --db \" + db_name + \\\n",
    "                  \" --collection \" + collection + \\\n",
    "                  \" --file \" + working_directory + json_file\n",
    "#print mongoimport_cmd \n",
    "\n",
    "# Before importing, drop collection if it exists\n",
    "if collection in db.collection_names():\n",
    "    print \"dropping collection\"\n",
    "    db[collection].drop()\n",
    "\n",
    "# Execute the command\n",
    "print \"Executing: \" + mongoimport_cmd\n",
    "subprocess.call(mongoimport_cmd.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Get the collection from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston_db = db[collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180637"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_db.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boston_db.distinct('created.user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Nodes and Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'multipolygon', u'count': 1},\n",
      " {u'_id': u'video', u'count': 1},\n",
      " {u'_id': u'chain_link', u'count': 1},\n",
      " {u'_id': u'way', u'count': 294189},\n",
      " {u'_id': u'Collaborative Program', u'count': 6},\n",
      " {u'_id': u'charter', u'count': 1},\n",
      " {u'_id': u'special', u'count': 1},\n",
      " {u'_id': u'Approved Special Education', u'count': 12},\n",
      " {u'_id': u'Academic', u'count': 34},\n",
      " {u'_id': u'Special-Law', u'count': 3},\n",
      " {u'_id': u'civil', u'count': 1},\n",
      " {u'_id': u'node', u'count': 1885954},\n",
      " {u'_id': u'Special', u'count': 49},\n",
      " {u'_id': u'Charter', u'count': 17},\n",
      " {u'_id': u'Public', u'count': 182},\n",
      " {u'_id': u'broad_leaved', u'count': 4},\n",
      " {u'_id': u'School', u'count': 77},\n",
      " {u'_id': u'private', u'count': 3},\n",
      " {u'_id': u'County', u'count': 3},\n",
      " {u'_id': u'Private', u'count': 87},\n",
      " {u'_id': u'State', u'count': 2},\n",
      " {u'_id': u'Special-Medical', u'count': 7},\n",
      " {u'_id': u'Special-Institutional', u'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "node_way = boston_db.aggregate([\n",
    "        {\"$group\" : {\"_id\" : \"$type\", \"count\" : {\"$sum\" : 1}}}])\n",
    "\n",
    "pprint.pprint(list(node_way))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1885954"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_db.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294189"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_db.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Contributing User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'crschmidt', u'count': 1229402}]\n"
     ]
    }
   ],
   "source": [
    "top_user = boston_db.aggregate([\n",
    "    {\"$match\":{\"type\":\"node\"}},\n",
    "    {\"$group\":{\"_id\":\"$created.user\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":1}\n",
    "])\n",
    "\n",
    "#print(list(top_user))\n",
    "pprint.pprint(list(top_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users having only 1 post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'num_users': 244, u'postcount': 1}]\n"
     ]
    }
   ],
   "source": [
    "type_buildings = boston_db.aggregate([\n",
    "    {\"$group\":{\"_id\":\"$created.user\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$group\":{\"_id\":{\"postcount\":\"$count\"},\"num_users\":{\"$sum\":1}}},\n",
    "    {\"$project\":{\"_id\":0,\"postcount\":\"$_id.postcount\",\"num_users\":1}},\n",
    "    {\"$sort\":{\"postcount\":1}},\n",
    "    {\"$limit\":1}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(type_buildings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Documents Containing a Street Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3026"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_db.find({\"address.street\" : {\"$exists\" : 1}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'02135', u'count': 250},\n",
      " {u'_id': u'02139', u'count': 230},\n",
      " {u'_id': u'02144', u'count': 92},\n",
      " {u'_id': u'02215', u'count': 61},\n",
      " {u'_id': u'02114', u'count': 59},\n",
      " {u'_id': u'02143', u'count': 52},\n",
      " {u'_id': u'02169', u'count': 52},\n",
      " {u'_id': u'02134', u'count': 49},\n",
      " {u'_id': u'02138', u'count': 49},\n",
      " {u'_id': u'02130', u'count': 44},\n",
      " {u'_id': u'02116', u'count': 41},\n",
      " {u'_id': u'02446', u'count': 38},\n",
      " {u'_id': u'02142', u'count': 31},\n",
      " {u'_id': u'02472', u'count': 30},\n",
      " {u'_id': u'02134-1307', u'count': 29},\n",
      " {u'_id': u'02210', u'count': 28},\n",
      " {u'_id': u'02467', u'count': 26},\n",
      " {u'_id': u'02155', u'count': 23},\n",
      " {u'_id': u'02474', u'count': 22},\n",
      " {u'_id': u'02128', u'count': 22},\n",
      " {u'_id': u'02145', u'count': 20},\n",
      " {u'_id': u'02132', u'count': 17},\n",
      " {u'_id': u'02141', u'count': 16},\n",
      " {u'_id': u'02111', u'count': 16},\n",
      " {u'_id': u'02149', u'count': 15},\n",
      " {u'_id': u'02140', u'count': 14},\n",
      " {u'_id': u'02108', u'count': 11},\n",
      " {u'_id': u'02134-1433', u'count': 11},\n",
      " {u'_id': u'02127', u'count': 11},\n",
      " {u'_id': u'02445', u'count': 11},\n",
      " {u'_id': u'02115', u'count': 9},\n",
      " {u'_id': u'02134-1420', u'count': 9},\n",
      " {u'_id': u'02478', u'count': 9},\n",
      " {u'_id': u'02134-1305', u'count': 9},\n",
      " {u'_id': u'02150', u'count': 8},\n",
      " {u'_id': u'02109', u'count': 8},\n",
      " {u'_id': u'02124', u'count': 8},\n",
      " {u'_id': u'02476', u'count': 8},\n",
      " {u'_id': u'02138-2903', u'count': 8},\n",
      " {u'_id': u'02131', u'count': 8},\n",
      " {u'_id': u'02186', u'count': 8},\n",
      " {u'_id': u'02138-2701', u'count': 8},\n",
      " {u'_id': u'02126', u'count': 7},\n",
      " {u'_id': u'02110', u'count': 7},\n",
      " {u'_id': u'02122', u'count': 6},\n",
      " {u'_id': u'02459', u'count': 6},\n",
      " {u'_id': u'02134-1322', u'count': 6},\n",
      " {u'_id': u'02118', u'count': 6},\n",
      " {u'_id': u'02136', u'count': 6},\n",
      " {u'_id': u'MA', u'count': 6},\n",
      " {u'_id': u'02134-1319', u'count': 5},\n",
      " {u'_id': u'02171', u'count': 5},\n",
      " {u'_id': u'02125', u'count': 5},\n",
      " {u'_id': u'02134-1442', u'count': 5},\n",
      " {u'_id': u'02138-2901', u'count': 4},\n",
      " {u'_id': u'02134-1311', u'count': 4},\n",
      " {u'_id': u'02138-2801', u'count': 4},\n",
      " {u'_id': u'02134-1321', u'count': 4},\n",
      " {u'_id': u'02458', u'count': 4},\n",
      " {u'_id': u'02134-1313', u'count': 4},\n",
      " {u'_id': u'02134-1409', u'count': 4},\n",
      " {u'_id': u'MA 02116', u'count': 4},\n",
      " {u'_id': u'02134-1317', u'count': 4},\n",
      " {u'_id': u'02151', u'count': 4},\n",
      " {u'_id': u'02138-2933', u'count': 3},\n",
      " {u'_id': u'02134-1316', u'count': 3},\n",
      " {u'_id': u'02138-2706', u'count': 3},\n",
      " {u'_id': u'02170', u'count': 3},\n",
      " {u'_id': u'02120', u'count': 3},\n",
      " {u'_id': u'02184', u'count': 3},\n",
      " {u'_id': u'02043', u'count': 3},\n",
      " {u'_id': u'02138-2736', u'count': 2},\n",
      " {u'_id': u'02134-1306', u'count': 2},\n",
      " {u'_id': u'02152', u'count': 2},\n",
      " {u'_id': u'02134-1318', u'count': 2},\n",
      " {u'_id': u'02129', u'count': 2},\n",
      " {u'_id': u'02131-3025', u'count': 2},\n",
      " {u'_id': u'02121', u'count': 2},\n",
      " {u'_id': u'02119', u'count': 2},\n",
      " {u'_id': u'02134-1312', u'count': 2},\n",
      " {u'_id': u'01250', u'count': 1},\n",
      " {u'_id': u'02138-2735', u'count': 1},\n",
      " {u'_id': u'02138-2763', u'count': 1},\n",
      " {u'_id': u'02138-2762', u'count': 1},\n",
      " {u'_id': u'02174', u'count': 1},\n",
      " {u'_id': u'01821', u'count': 1},\n",
      " {u'_id': u'02134-1327', u'count': 1},\n",
      " {u'_id': u'02130-4803', u'count': 1},\n",
      " {u'_id': u'02131-4931', u'count': 1},\n",
      " {u'_id': u'01125', u'count': 1},\n",
      " {u'_id': u'02140-2215', u'count': 1},\n",
      " {u'_id': u'02474-8735', u'count': 1},\n",
      " {u'_id': u'02138-2742', u'count': 1},\n",
      " {u'_id': u'02138-3003', u'count': 1},\n",
      " {u'_id': u'02445-5841', u'count': 1},\n",
      " {u'_id': u'02138-3824', u'count': 1},\n",
      " {u'_id': u'02138-1901', u'count': 1},\n",
      " {u'_id': u'02284-6028', u'count': 1},\n",
      " {u'_id': u'02140-1340', u'count': 1},\n",
      " {u'_id': u'01238', u'count': 1},\n",
      " {u'_id': u'02110-1301', u'count': 1},\n",
      " {u'_id': u'02132-3226', u'count': 1},\n",
      " {u'_id': u'01240', u'count': 1},\n",
      " {u'_id': u'02026-5036', u'count': 1},\n",
      " {u'_id': u'01944', u'count': 1},\n",
      " {u'_id': u'MA 02186', u'count': 1},\n",
      " {u'_id': u'02113', u'count': 1},\n",
      " {u'_id': u'Mass Ave', u'count': 1},\n",
      " {u'_id': u'02205', u'count': 1},\n",
      " {u'_id': u' 02472', u'count': 1},\n",
      " {u'_id': u'02136-2460', u'count': 1},\n",
      " {u'_id': u'02132-1239', u'count': 1},\n",
      " {u'_id': u'02159', u'count': 1},\n",
      " {u'_id': u'02114-3203', u'count': 1},\n",
      " {u'_id': u'02228', u'count': 1},\n",
      " {u'_id': u'02138-2724', u'count': 1},\n",
      " {u'_id': u'02148', u'count': 1},\n",
      " {u'_id': u'02026', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "zipcodes = boston_db.aggregate([\n",
    "        {\"$match\" : {\"address.postcode\" : {\"$exists\" : 1}}}, \\\n",
    "        {\"$group\" : {\"_id\" : \"$address.postcode\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}}])\n",
    "\n",
    "#for document in zipcodes:\n",
    "#    print(document)\n",
    "    \n",
    "pprint.pprint(list(zipcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Most Common Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Boston', u'count': 594},\n",
      " {u'_id': u'Malden', u'count': 413},\n",
      " {u'_id': u'Cambridge', u'count': 323},\n",
      " {u'_id': u'Somerville', u'count': 153},\n",
      " {u'_id': u'Quincy', u'count': 51}]\n"
     ]
    }
   ],
   "source": [
    "cities = boston_db.aggregate([{\"$match\" : {\"address.city\" : {\"$exists\" : 1}}}, \\\n",
    "                           {\"$group\" : {\"_id\" : \"$address.city\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                           {\"$sort\" : {\"count\" : -1}}, \\\n",
    "                           {\"$limit\" : 5}])\n",
    "\n",
    "#for city in cities :\n",
    "#    print city\n",
    "    \n",
    "pprint.pprint(list(cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 1192},\n",
      " {u'_id': u'bench', u'count': 946},\n",
      " {u'_id': u'school', u'count': 774},\n",
      " {u'_id': u'restaurant', u'count': 532},\n",
      " {u'_id': u'parking_space', u'count': 444},\n",
      " {u'_id': u'place_of_worship', u'count': 407},\n",
      " {u'_id': u'library', u'count': 342},\n",
      " {u'_id': u'bicycle_parking', u'count': 238},\n",
      " {u'_id': u'cafe', u'count': 199},\n",
      " {u'_id': u'fast_food', u'count': 169}]\n"
     ]
    }
   ],
   "source": [
    "amenities = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : {\"$exists\" : 1}}}, \\\n",
    "        {\"$group\" : {\"_id\" : \"$amenity\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}}, \\\n",
    "        {\"$limit\" : 10}])\n",
    "\n",
    "#for document in amenities:\n",
    "#    print document\n",
    "\n",
    "pprint.pprint(list(amenities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'bench', u'count': 938},\n",
      " {u'_id': u'restaurant', u'count': 493},\n",
      " {u'_id': u'school', u'count': 370},\n",
      " {u'_id': u'place_of_worship', u'count': 317},\n",
      " {u'_id': u'bicycle_parking', u'count': 238},\n",
      " {u'_id': u'cafe', u'count': 182},\n",
      " {u'_id': u'fast_food', u'count': 154},\n",
      " {u'_id': u'library', u'count': 141},\n",
      " {u'_id': u'bicycle_rental', u'count': 141},\n",
      " {u'_id': u'fire_station', u'count': 96}]\n"
     ]
    }
   ],
   "source": [
    "amenities = boston_db.aggregate([\n",
    "    {\"$match\":{\"amenity\":{\"$exists\":1},\"type\":\"node\"}},\n",
    "    {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":10}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(amenities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common building types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'yes', u'count': 248378},\n",
      " {u'_id': u'garage', u'count': 673},\n",
      " {u'_id': u'house', u'count': 629},\n",
      " {u'_id': u'apartments', u'count': 422},\n",
      " {u'_id': u'university', u'count': 295},\n",
      " {u'_id': u'shed', u'count': 131},\n",
      " {u'_id': u'roof', u'count': 109},\n",
      " {u'_id': u'dormitory', u'count': 98},\n",
      " {u'_id': u'school', u'count': 88},\n",
      " {u'_id': u'residential', u'count': 76},\n",
      " {u'_id': u'commercial', u'count': 69},\n",
      " {u'_id': u'retail', u'count': 53},\n",
      " {u'_id': u'entrance', u'count': 37},\n",
      " {u'_id': u'storage_tank', u'count': 30},\n",
      " {u'_id': u'church', u'count': 27},\n",
      " {u'_id': u'home', u'count': 26},\n",
      " {u'_id': u'office', u'count': 20},\n",
      " {u'_id': u'industrial', u'count': 16},\n",
      " {u'_id': u'hospital', u'count': 7},\n",
      " {u'_id': u'hotel', u'count': 5}]\n"
     ]
    }
   ],
   "source": [
    "type_buildings = boston_db.aggregate([\n",
    "    {'$match': {'building': {'$exists': 1}}}, \n",
    "    {'$group': { '_id': '$building','count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}}, {'$limit': 20}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(type_buildings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Religions with Denominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': {u'religion': u'christian'}, u'count': 189},\n",
      " {u'_id': {u'denomination': u'baptist', u'religion': u'christian'},\n",
      "  u'count': 53},\n",
      " {u'_id': {u'denomination': u'methodist', u'religion': u'christian'},\n",
      "  u'count': 22},\n",
      " {u'_id': {u'denomination': u'catholic', u'religion': u'christian'},\n",
      "  u'count': 22},\n",
      " {u'_id': {}, u'count': 16},\n",
      " {u'_id': {u'denomination': u'roman_catholic', u'religion': u'christian'},\n",
      "  u'count': 10},\n",
      " {u'_id': {u'denomination': u'presbyterian', u'religion': u'christian'},\n",
      "  u'count': 10},\n",
      " {u'_id': {u'denomination': u'lutheran', u'religion': u'christian'},\n",
      "  u'count': 9},\n",
      " {u'_id': {u'religion': u'jewish'}, u'count': 9},\n",
      " {u'_id': {u'denomination': u'pentecostal', u'religion': u'christian'},\n",
      "  u'count': 8},\n",
      " {u'_id': {u'denomination': u'episcopal', u'religion': u'christian'},\n",
      "  u'count': 8},\n",
      " {u'_id': {u'religion': u'unitarian'}, u'count': 6},\n",
      " {u'_id': {u'denomination': u'orthodox', u'religion': u'christian'},\n",
      "  u'count': 4},\n",
      " {u'_id': {u'religion': u'unitarian_universalist'}, u'count': 3},\n",
      " {u'_id': {u'denomination': u'mormon', u'religion': u'christian'},\n",
      "  u'count': 3},\n",
      " {u'_id': {u'denomination': u'anglican', u'religion': u'christian'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'denomination': u'zen', u'religion': u'buddhist'}, u'count': 2},\n",
      " {u'_id': {u'denomination': u'congregational', u'religion': u'christian'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'denomination': u'reform', u'religion': u'jewish'}, u'count': 2},\n",
      " {u'_id': {u'denomination': u'reformed', u'religion': u'jewish'}, u'count': 2},\n",
      " {u'_id': {u'denomination': u'jehovahs_witness', u'religion': u'christian'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'denomination': u'evangelical', u'religion': u'christian'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'denomination': u'non-denominational', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'seventh_day_adventist',\n",
      "           u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'greek_orthodox', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'conservative', u'religion': u'jewish'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'union church of christ',\n",
      "           u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'Congregational', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'sunni', u'religion': u'muslim'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'UUA', u'religion': u'christian'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'united_church_of_christ',\n",
      "           u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'salvation_army', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'christ_scientist', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'quaker', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'swedenborgian', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'hasidic', u'religion': u'jewish'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'roman_catholic'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'greek_catholic', u'religion': u'christian'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'religion': u'buddhist'}, u'count': 1},\n",
      " {u'_id': {u'religion': u'muslim'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'orthodox', u'religion': u'jewish'}, u'count': 1},\n",
      " {u'_id': {u'denomination': u'non-denominational', u'religion': u'jewish'},\n",
      "  u'count': 1},\n",
      " {u'_id': {u'denomination': u'unitarian', u'religion': u'christian'},\n",
      "  u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "religions = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : \"place_of_worship\"}}, \\\n",
    "        {\"$group\" : {\"_id\" : {\"religion\" : \"$religion\", \"denomination\" : \"$denomination\"}, \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}}])\n",
    "\n",
    "#for document in religions:\n",
    "#    print document\n",
    "\n",
    "pprint.pprint(list(religions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Leisures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'park', u'count': 750},\n",
      " {u'_id': u'pitch', u'count': 545},\n",
      " {u'_id': u'recreation_ground', u'count': 350},\n",
      " {u'_id': u'playground', u'count': 334},\n",
      " {u'_id': u'nature_reserve', u'count': 105},\n",
      " {u'_id': u'garden', u'count': 35},\n",
      " {u'_id': u'sports_centre', u'count': 33},\n",
      " {u'_id': u'picnic_table', u'count': 33},\n",
      " {u'_id': u'swimming_pool', u'count': 28},\n",
      " {u'_id': u'common', u'count': 19}]\n"
     ]
    }
   ],
   "source": [
    "leisures = boston_db.aggregate([{\"$match\" : {\"leisure\" : {\"$exists\" : 1}}}, \\\n",
    "                           {\"$group\" : {\"_id\" : \"$leisure\", \"count\" : {\"$sum\" : 1}}}, \\\n",
    "                           {\"$sort\" : {\"count\" : -1}}, \\\n",
    "                           {\"$limit\" : 10}])\n",
    "\n",
    "#for document in leisures:\n",
    "#    print document\n",
    "\n",
    "pprint.pprint(list(leisures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': {u'name': u'Boston University'}, u'count': 41},\n",
      " {u'_id': {u'name': u'Massachusetts Institute of Technology'}, u'count': 10},\n",
      " {u'_id': {u'name': u'Suffolk University'}, u'count': 8},\n",
      " {u'_id': {u'name': u'Harvard University'}, u'count': 6},\n",
      " {u'_id': {u'name': None}, u'count': 4},\n",
      " {u'_id': {u'name': u'University of Massachusetts Boston'}, u'count': 3},\n",
      " {u'_id': {u'name': u'Boston University Medical Campus'}, u'count': 3},\n",
      " {u'_id': {u'name': u'University Hall'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Harvard Medical School'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Benjamin Franklin Institute of Technology'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'name': u'Littauer Center'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Northeastern University'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Boston College'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Radcliffe Gym'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Agassiz House'}, u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "universities = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : \"university\"}}, \\\n",
    "        {\"$group\" : {\"_id\" : {\"name\" : \"$name\"}, \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\":15}\n",
    "    ])\n",
    "\n",
    "pprint.pprint(list(universities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': {u'name': None}, u'count': 13},\n",
      " {u'_id': {u'name': u'Milton Academy'}, u'count': 4},\n",
      " {u'_id': {u'name': u'Lincoln School'}, u'count': 3},\n",
      " {u'_id': {u'name': u'Boston Community Leadership Academy'}, u'count': 3},\n",
      " {u'_id': {u'name': u'New Mission High School'}, u'count': 3},\n",
      " {u'_id': {u'name': u'Dexter School'}, u'count': 3},\n",
      " {u'_id': {u'name': u'Boston Middle School Academy'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Clark Avenue School'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Kennedy Day School'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Phillips School'}, u'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "schools = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : \"school\"}}, \\\n",
    "        {\"$group\" : {\"_id\" : {\"name\" : \"$name\"}, \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\":10}\n",
    "    ])\n",
    "\n",
    "pprint.pprint(list(schools))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Prisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': {u'name': u'Norfolk County Jail'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Middlesex County Jail (Cambridge)'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Suffolk County House of Correction'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Suffolk County Jail'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Boston Pre-Release Center'}, u'count': 1},\n",
      " {u'_id': {u'name': u'Lemuel Shattuck Hospital Correctional Unit'},\n",
      "  u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "prisons = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : \"prison\"}}, \\\n",
    "        {\"$group\" : {\"_id\" : {\"name\" : \"$name\"}, \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}}])\n",
    "\n",
    "pprint.pprint(list(prisons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': {u'name': u'Carney Hospital'}, u'count': 3},\n",
      " {u'_id': {u'name': None}, u'count': 2},\n",
      " {u'_id': {u'name': u\"St. Elizabeth's Medical Center\"}, u'count': 2},\n",
      " {u'_id': {u'name': u'Arbour Hospital'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Bournewood Hospital'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Arbour-Hri Hospital'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Cambridge Health Alliance-Whidden Memorial Hospital'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'name': u'Faulkner Hospital'}, u'count': 2},\n",
      " {u'_id': {u'name': u'Steward Satellite Emergency Facility - Quincy'},\n",
      "  u'count': 2},\n",
      " {u'_id': {u'name': u'Central Street Health Center'}, u'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "hospitals = boston_db.aggregate([\n",
    "        {\"$match\" : {\"amenity\" : \"hospital\"}}, \\\n",
    "        {\"$group\" : {\"_id\" : {\"name\" : \"$name\"}, \"count\" : {\"$sum\" : 1}}}, \\\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\":10}\n",
    "    ])\n",
    "\n",
    "\n",
    "pprint.pprint(list(hospitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular cuisines in fast foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fast_food = boston_db.aggregate([\n",
    "    {\"$match\":{\"cuisine\":{\"$exists\":1},\"amenity\":\"fast_food\"}},\n",
    "    {\"$group\":{\"_id\":\"$cuisine\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":10}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(fast_food))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular gas station brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Gulf', u'count': 4},\n",
      " {u'_id': u'Shell', u'count': 3},\n",
      " {u'_id': u'Hess', u'count': 3},\n",
      " {u'_id': u'Super Petroleum', u'count': 1},\n",
      " {u'_id': u\"Eli's\", u'count': 1},\n",
      " {u'_id': u'APrime Energy', u'count': 1},\n",
      " {u'_id': u'US Petroleum', u'count': 1},\n",
      " {u'_id': u'Cumberland Farm', u'count': 1},\n",
      " {u'_id': u'Valvoline Oil Change', u'count': 1},\n",
      " {u'_id': u'Sunoco', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "gas_station_brands = boston_db.aggregate([\n",
    "    {\"$match\":{\"brand\":{\"$exists\":1},\"amenity\":\"fuel\"}},\n",
    "    {\"$group\":{\"_id\":\"$brand\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":10}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(gas_station_brands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Bank of America', u'count': 11},\n",
      " {u'_id': u'Citizens Bank', u'count': 7},\n",
      " {u'_id': u'TD Bank', u'count': 6},\n",
      " {u'_id': u'Eastern Bank', u'count': 4},\n",
      " {u'_id': u'Cambridge Savings Bank', u'count': 4},\n",
      " {u'_id': u'Santander', u'count': 4},\n",
      " {u'_id': u'Sovereign Bank', u'count': 4},\n",
      " {u'_id': u'Brookline Bank', u'count': 3},\n",
      " {u'_id': u'East Cambridge Savings Bank', u'count': 2},\n",
      " {u'_id': u'Cambridge Trust Company', u'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "banks = boston_db.aggregate([\n",
    "    {\"$match\":{\"name\":{\"$exists\":1},\"amenity\":\"bank\"}},\n",
    "    {\"$group\":{\"_id\":\"$name\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":10}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(banks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Panera Bread', u'count': 6},\n",
      " {u'_id': u\"Bertucci's\", u'count': 4},\n",
      " {u'_id': u'Dunkin Donuts', u'count': 2},\n",
      " {u'_id': u'Olecito', u'count': 2},\n",
      " {u'_id': u'The Elephant Walk', u'count': 2},\n",
      " {u'_id': u'Chipotle', u'count': 2},\n",
      " {u'_id': u'Boloco', u'count': 2},\n",
      " {u'_id': u\"Crazy Dough's\", u'count': 2},\n",
      " {u'_id': u'Ninety Nine', u'count': 2},\n",
      " {u'_id': u'Boca Grande', u'count': 2}]\n"
     ]
    }
   ],
   "source": [
    "restaurants = boston_db.aggregate([\n",
    "    {\"$match\":{\"name\":{\"$exists\":1},\"amenity\":\"restaurant\"}},\n",
    "    {\"$group\":{\"_id\":\"$name\",\"count\":{\"$sum\":1}}},\n",
    "    {\"$sort\":{\"count\":-1}},\n",
    "    {\"$limit\":10}\n",
    "])\n",
    "\n",
    "pprint.pprint(list(restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Analyzing the data of Boston I found out that not all nodes or ways include this information since its geographical position is represented within regions of a city. What could be done in this case, is check if each node or way belongs to a city based on the latitude and longitude and ensure that the property \"address.city\" is properly informed. By doing so, we could get statistics related to cities in a much more reliable way. In fact, I think this is the biggest benefit to anticipate problems and implement improvements to the data you want to analyze. Real world data are very susceptible to being incomplete, noisy and inconsistent which means that if you have low-quality of data the results of their analysis will also be of poor quality.\n",
    "\n",
    "  I think that extending this open source project to include data such as user reviews of establishments, subjective areas of what bound a good and bad neighborhood, housing price data, school reviews, walkability/bikeability, quality of mass transit, and on would form a solid foundation of robust recommender systems. These recommender systems could aid users in anything from finding a new home or apartment to helping a user decide where to spend a weekend afternoon.\n",
    "  \n",
    "Another alternative to help in the absence of information in the region would be the use of gamification or crowdsource information to make more people help in the map contribution. Something like the mobile apps similar to Waze and Minutely have already done to make the users responsible for improving the app and  social network around the app.\n",
    "\n",
    "\n",
    " A different application of this project is that it can be helpful on the problem of how the city boundaries well-defined. The transportation networks (street networks), the built environment can be good indicators of metropolitan area and combining an elementary clustering technique, we consider two street intersections to belong to the same cluster if they have a distance below a given distance threshold (in metres). The geospatial information gives us a good definition of city boundaries through spatial urban networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting fact that we can use the geospatial coordinates information to find out country/city name (search OSM data by name and address and to generate synthetic addresses of OSM points). This problem is called reverse geocoding which  maps geospatial coordinates to location name. And the <a href=\"http://wiki.openstreetmap.org/wiki/Nominatim#Reverse_Geocoding\">Nominatim</a> from Open Street Map enables us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Garage, Mount Auburn Street, Harvard Square, Cambridge, Middlesex County, Massachusetts, 02138, United States of America\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "location = geolocator.reverse(\"42.3725677, -71.1193068\")\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, potential problems associated with reverse geocoding is that it may give us weird results near the poles and the international date line or for cities within cities, for example certain locations in Rome may return \"Vatican City\" - depending on the lat/lon specified in the database for each\n",
    "\n",
    "For example :  Pontificio Collegio Teutonico di Santa Maria in Campo Santo (Collegio Teutonico) is located in Vatican City but the result of given set of coordinates gives us the location in Roma, Italia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sotto La Cupola - Guest House, 15, Via Cardinal Agliardi, Aurelio, Municipio Roma XIII, Roma, RM, LAZ, 00165, Italia\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "vatican=(41.89888433, 12.45376451)\n",
    "location = geolocator.reverse(vatican)\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "artic=(-86.06303611,6.81517107)\n",
    "location = geolocator.reverse(artic)\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the many issues with the reverse coding, I think another benefits of this project that it can be applied in disease mapping which facilitates us use the longitudes and latitudes information to find the plaintext addresses of patients for identifying patterns, correlates, and predictors of disease in academia, government and private sector with the widespread availability of geographic information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Conclusion         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review of the data is cursory, but it seems that the Boston area is incomplete, though I believe it has been well cleaned and represented after this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://wiki.openstreetmap.org/wiki/Main_Page\">OpenStreetMap Wiki Page</a>\n",
    "\n",
    "<a href=\"https://wiki.openstreetmap.org/wiki/OSM_XML\">OpenStreetMap Wiki Page - OSM XML\n",
    "</a>\n",
    "\n",
    "<a href=\"http://wiki.openstreetmap.org/wiki/Map_Features\">OpenStreetMap Map Features</a>\n",
    "\n",
    "<a href=\"https://docs.python.org/2/library/re.html#search-vs-match\">Python Regular Expressions</a>\n",
    "\n",
    "<a href=\"https://docs.mongodb.org/v2.4/reference/operator/\">MongoDB Operators</a>\n",
    "\n",
    "<a href=\"http://www.choskim.me/how-to-install-mongodb-on-apples-mac-os-x/\">Install MongoDB on Apple's Mac OS X</a>\n",
    "\n",
    "<a href=\"https://books.google.com/books?id=_VkrAQAAQBAJ&pg=PA241&lpg=PA241&dq=execute+mongodb+command+in+ipython&source=bl&ots=JqnwlwRvkN&sig=h-TrwspKAmHt1g1ELItnWkDmRHs&hl=en&sa=X&ved=0ahUKEwiJnaiikIrLAhUM8CYKHZ8mBrcQ6AEILzAD#v=onepage&q=execute%20mongodb%20command%20in%20ipython&f=false/\">Install MongoDB</a>\n",
    "\n",
    "<a href=\"http://michaelcrump.net/how-to-run-html-files-in-your-browser-from-github/\"> Run HTML files in your Browser from GitHub </a>\n",
    "\n",
    "<a href=\"http://eberlitz.github.io/2015/09/18/data-wrangle-openstreetmaps-data/\">Data Wrangling OpenStreetMap 1</a>\n",
    "\n",
    "<a href=\"https://htmlpreview.github.io/?https://github.com/jdamiani27/Data-Wrangling-with-MongoDB/blob/master/Final_Project/OSM_wrangling.html#Top-Contributing-User\">Data Wrangling OpenStreetMap 2</a>\n",
    "\n",
    "\n",
    "<a href=\"http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country\">Find the city from lat-long coordinates</a>\n",
    "\n",
    "\n",
    "<a href=\"http://ij-healthgeographics.biomedcentral.com/articles/10.1186/1476-072X-5-56\">Disease Mapping</a>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://myadventuresincoding.wordpress.com/2011/10/02/mongodb-geospatial-queries/\">Mongodb geospatial queries</a>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://tugdualgrall.blogspot.com/2014/08/introduction-to-mongodb-geospatial.html\">Intro to mongodb geospatial</a>\n",
    "\n",
    "<a href=\"http://www.longitude-latitude-maps.com/city/231_0,Vatican+City,Vatican+City\">Long Lat Maps</a>\n",
    "\n",
    "\n",
    "<a href=\"http://www.spatialcomplexity.info/files/2015/10/BATTY-JRSInterface-2015.pdf\">City Boundaries</a>\n",
    "\n",
    "\n",
    "<a href=\"http://www.innovation-cities.com/how-do-you-define-a-city-4-definitions-of-city-boundaries/1314\">City Boundaries 2</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.killpg(pro.pid, signal.SIGTERM)  # Send the signal to all the process groups, killing the MongoDB instance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
